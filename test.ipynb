{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3075aa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_attention_pattern_symmetry (__main__.TestScaledDotProductAttention)\n",
      "Test t√≠nh ch·∫•t ƒë·ªëi x·ª©ng khi Q=K ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE TEST FOR PURE SCALED DOT-PRODUCT ATTENTION\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAIL\n",
      "test_attention_weights_properties (__main__.TestScaledDotProductAttention)\n",
      "Test c√°c t√≠nh ch·∫•t c·ªßa attention weights ... ok\n",
      "test_batch_consistency (__main__.TestScaledDotProductAttention)\n",
      "Test t√≠nh nh·∫•t qu√°n gi·ªØa c√°c batch ... ok\n",
      "test_causal_mask (__main__.TestScaledDotProductAttention)\n",
      "Test v·ªõi causal mask (cho decoder self-attention) ... ok\n",
      "test_deterministic_output (__main__.TestScaledDotProductAttention)\n",
      "Test t√≠nh deterministic ... ok\n",
      "test_forward_shape_cross_attention (__main__.TestScaledDotProductAttention)\n",
      "Test shape v·ªõi cross-attention (K,V c√≥ seq_len kh√°c Q) ... ok\n",
      "test_forward_shape_self_attention (__main__.TestScaledDotProductAttention)\n",
      "Test shape v·ªõi self-attention (Q=K=V) ... ok\n",
      "test_gradient_flow (__main__.TestScaledDotProductAttention)\n",
      "Test gradient flow qua attention mechanism ... ok\n",
      "test_initialization (__main__.TestScaledDotProductAttention)\n",
      "Test kh·ªüi t·∫°o model ... ok\n",
      "test_numerical_stability (__main__.TestScaledDotProductAttention)\n",
      "Test t√≠nh ·ªïn ƒë·ªãnh s·ªë h·ªçc v·ªõi gi√° tr·ªã extreme ... ok\n",
      "test_padding_mask (__main__.TestScaledDotProductAttention)\n",
      "Test v·ªõi padding mask ... ok\n",
      "test_scaling_effect (__main__.TestScaledDotProductAttention)\n",
      "Test hi·ªáu ·ª©ng c·ªßa scaling factor sqrt(d_k) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_attention_pattern_symmetry (__main__.TestScaledDotProductAttention)\n",
      "Test t√≠nh ch·∫•t ƒë·ªëi x·ª©ng khi Q=K\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_17109/946854499.py\", line 232, in test_attention_pattern_symmetry\n",
      "    self.assertGreaterEqual(diagonal_vals.mean().item(), off_diagonal_vals.mean().item())\n",
      "AssertionError: 0.18042083084583282 not greater than or equal to 0.2048948109149933\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 1.967s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST SUMMARY\n",
      "======================================================================\n",
      "Tests run: 12\n",
      "Failures: 1\n",
      "Errors: 0\n",
      "Success rate: 91.7%\n",
      "\n",
      "======================================================================\n",
      "ATTENTION MECHANISM DEMONSTRATION\n",
      "======================================================================\n",
      "Input shape: torch.Size([1, 5, 64])\n",
      "\n",
      "--- SELF-ATTENTION ---\n",
      "Output shape: torch.Size([1, 5, 64])\n",
      "Attention weights shape: torch.Size([1, 5, 5])\n",
      "Attention matrix:\n",
      "  Position 0: [0.256 0.132 0.186 0.240 0.186]\n",
      "  Position 1: [0.171 0.176 0.243 0.149 0.261]\n",
      "  Position 2: [0.233 0.178 0.196 0.182 0.211]\n",
      "  Position 3: [0.173 0.213 0.175 0.282 0.156]\n",
      "  Position 4: [0.237 0.163 0.094 0.170 0.336]\n",
      "\n",
      "--- CAUSAL MASKED ATTENTION ---\n",
      "Masked attention matrix:\n",
      "  Position 0: [1.000 0.000 0.000 0.000 0.000]\n",
      "  Position 1: [0.494 0.506 0.000 0.000 0.000]\n",
      "  Position 2: [0.384 0.293 0.323 0.000 0.000]\n",
      "  Position 3: [0.205 0.253 0.207 0.335 0.000]\n",
      "  Position 4: [0.237 0.163 0.094 0.170 0.336]\n",
      "\n",
      "--- CROSS-ATTENTION ---\n",
      "Cross-attention output shape: torch.Size([1, 5, 64])\n",
      "Cross-attention weights shape: torch.Size([1, 5, 3])\n",
      "‚úì Demonstration completed successfully!\n",
      "\n",
      "‚ùå Some tests failed. Please check the output above.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Linear transformations cho Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pure Scaled Dot-Product Attention\n",
    "        \n",
    "        Args:\n",
    "            Q: Query tensor (batch_size, seq_len_q, d_model)\n",
    "            K: Key tensor (batch_size, seq_len_k, d_model)  \n",
    "            V: Value tensor (batch_size, seq_len_v, d_model)\n",
    "            mask: Optional mask tensor (batch_size, seq_len_q, seq_len_k) ho·∫∑c broadcastable\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output (batch_size, seq_len_q, d_model)\n",
    "        \"\"\"\n",
    "        # Linear transformations\n",
    "        Q = self.W_q(Q)  # (batch_size, seq_len_q, d_model)\n",
    "        K = self.W_k(K)  # (batch_size, seq_len_k, d_model)\n",
    "        V = self.W_v(V)  # (batch_size, seq_len_v, d_model)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        d_k = K.size(-1)  # d_model\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # scores shape: (batch_size, seq_len_q, seq_len_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        # attention_weights shape: (batch_size, seq_len_q, seq_len_k)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        # output shape: (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forward_with_weights(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass v·ªõi return c·∫£ attention weights (ƒë·ªÉ debugging/visualization)\n",
    "        \"\"\"\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        d_k = K.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class TestScaledDotProductAttention(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Thi·∫øt l·∫≠p c√°c tham s·ªë test\"\"\"\n",
    "        self.batch_size = 2\n",
    "        self.seq_len = 10\n",
    "        self.d_model = 512\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # T·∫°o model\n",
    "        self.model = ScaledDotProductAttention(self.d_model).to(self.device)\n",
    "        \n",
    "        # T·∫°o input tensors\n",
    "        self.Q = torch.randn(self.batch_size, self.seq_len, self.d_model).to(self.device)\n",
    "        self.K = torch.randn(self.batch_size, self.seq_len, self.d_model).to(self.device)\n",
    "        self.V = torch.randn(self.batch_size, self.seq_len, self.d_model).to(self.device)\n",
    "    \n",
    "    def test_initialization(self):\n",
    "        \"\"\"Test kh·ªüi t·∫°o model\"\"\"\n",
    "        model = ScaledDotProductAttention(512)\n",
    "        self.assertEqual(model.d_model, 512)\n",
    "        self.assertIsInstance(model.W_q, nn.Linear)\n",
    "        self.assertIsInstance(model.W_k, nn.Linear)\n",
    "        self.assertIsInstance(model.W_v, nn.Linear)\n",
    "        \n",
    "        # Ki·ªÉm tra dimensions c·ªßa linear layers\n",
    "        self.assertEqual(model.W_q.in_features, 512)\n",
    "        self.assertEqual(model.W_q.out_features, 512)\n",
    "    \n",
    "    def test_forward_shape_self_attention(self):\n",
    "        \"\"\"Test shape v·ªõi self-attention (Q=K=V)\"\"\"\n",
    "        X = torch.randn(self.batch_size, self.seq_len, self.d_model).to(self.device)\n",
    "        output = self.model(X, X, X)\n",
    "        expected_shape = (self.batch_size, self.seq_len, self.d_model)\n",
    "        self.assertEqual(output.shape, expected_shape)\n",
    "    \n",
    "    def test_forward_shape_cross_attention(self):\n",
    "        \"\"\"Test shape v·ªõi cross-attention (K,V c√≥ seq_len kh√°c Q)\"\"\"\n",
    "        seq_len_kv = 15\n",
    "        K_cross = torch.randn(self.batch_size, seq_len_kv, self.d_model).to(self.device)\n",
    "        V_cross = torch.randn(self.batch_size, seq_len_kv, self.d_model).to(self.device)\n",
    "        \n",
    "        output = self.model(self.Q, K_cross, V_cross)\n",
    "        expected_shape = (self.batch_size, self.seq_len, self.d_model)  # shape theo Q\n",
    "        self.assertEqual(output.shape, expected_shape)\n",
    "    \n",
    "    def test_attention_weights_properties(self):\n",
    "        \"\"\"Test c√°c t√≠nh ch·∫•t c·ªßa attention weights\"\"\"\n",
    "        output, attention_weights = self.model.forward_with_weights(self.Q, self.K, self.V)\n",
    "        \n",
    "        # 1. Shape c·ªßa attention weights\n",
    "        expected_weights_shape = (self.batch_size, self.seq_len, self.seq_len)\n",
    "        self.assertEqual(attention_weights.shape, expected_weights_shape)\n",
    "        \n",
    "        # 2. Attention weights ph·∫£i sum to 1 theo dim cu·ªëi\n",
    "        weights_sum = attention_weights.sum(dim=-1)\n",
    "        expected_sum = torch.ones(self.batch_size, self.seq_len).to(self.device)\n",
    "        torch.testing.assert_close(weights_sum, expected_sum, rtol=1e-5, atol=1e-6)\n",
    "        \n",
    "        # 3. Attention weights ph·∫£i >= 0\n",
    "        self.assertTrue((attention_weights >= 0).all())\n",
    "        \n",
    "        # 4. Attention weights ph·∫£i <= 1\n",
    "        self.assertTrue((attention_weights <= 1).all())\n",
    "    \n",
    "    def test_causal_mask(self):\n",
    "        \"\"\"Test v·ªõi causal mask (cho decoder self-attention)\"\"\"\n",
    "        # T·∫°o causal mask (lower triangular)\n",
    "        mask = torch.tril(torch.ones(self.seq_len, self.seq_len))\n",
    "        mask = mask.unsqueeze(0).expand(self.batch_size, -1, -1).to(self.device)\n",
    "        \n",
    "        output, attention_weights = self.model.forward_with_weights(self.Q, self.K, self.V, mask=mask)\n",
    "        \n",
    "        # Ki·ªÉm tra attention weights = 0 ·ªü v·ªã tr√≠ mask = 0\n",
    "        masked_positions = (mask == 0)\n",
    "        masked_weights = attention_weights[masked_positions]\n",
    "        \n",
    "        # Attention weights t·∫°i v·ªã tr√≠ masked ph·∫£i r·∫•t nh·ªè (‚âà 0)\n",
    "        self.assertTrue((masked_weights < 1e-8).all())\n",
    "    \n",
    "    def test_padding_mask(self):\n",
    "        \"\"\"Test v·ªõi padding mask\"\"\"\n",
    "        # Gi·∫£ s·ª≠ token cu·ªëi c√πng l√† padding\n",
    "        mask = torch.ones(self.batch_size, self.seq_len, self.seq_len).to(self.device)\n",
    "        mask[:, :, -1] = 0  # Mask token cu·ªëi c√πng\n",
    "        \n",
    "        output, attention_weights = self.model.forward_with_weights(self.Q, self.K, self.V, mask=mask)\n",
    "        \n",
    "        # Attention weights t·∫°i c·ªôt cu·ªëi ph·∫£i ‚âà 0\n",
    "        last_column_weights = attention_weights[:, :, -1]\n",
    "        self.assertTrue((last_column_weights < 1e-8).all())\n",
    "    \n",
    "    def test_scaling_effect(self):\n",
    "        \"\"\"Test hi·ªáu ·ª©ng c·ªßa scaling factor sqrt(d_k)\"\"\"\n",
    "        # So s√°nh v·ªõi attention kh√¥ng scale\n",
    "        Q_proj = self.model.W_q(self.Q)\n",
    "        K_proj = self.model.W_k(self.K)\n",
    "        V_proj = self.model.W_v(self.V)\n",
    "        \n",
    "        # Attention c√≥ scale\n",
    "        scores_scaled = torch.matmul(Q_proj, K_proj.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "        weights_scaled = F.softmax(scores_scaled, dim=-1)\n",
    "        \n",
    "        # Attention kh√¥ng scale\n",
    "        scores_unscaled = torch.matmul(Q_proj, K_proj.transpose(-2, -1))\n",
    "        weights_unscaled = F.softmax(scores_unscaled, dim=-1)\n",
    "        \n",
    "        # Scaled attention weights n√™n √≠t concentrated h∆°n (entropy cao h∆°n)\n",
    "        def entropy(weights):\n",
    "            return -(weights * torch.log(weights + 1e-9)).sum(dim=-1)\n",
    "        \n",
    "        entropy_scaled = entropy(weights_scaled).mean()\n",
    "        entropy_unscaled = entropy(weights_unscaled).mean()\n",
    "        \n",
    "        # V·ªõi d_model l·ªõn, scaled attention th∆∞·ªùng c√≥ entropy cao h∆°n\n",
    "        if self.d_model > 64:\n",
    "            self.assertGreater(entropy_scaled, entropy_unscaled)\n",
    "    \n",
    "    def test_gradient_flow(self):\n",
    "        \"\"\"Test gradient flow qua attention mechanism\"\"\"\n",
    "        self.Q.requires_grad_(True)\n",
    "        self.K.requires_grad_(True)\n",
    "        self.V.requires_grad_(True)\n",
    "        \n",
    "        output = self.model(self.Q, self.K, self.V)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Ki·ªÉm tra gradients kh√¥ng ph·∫£i None v√† kh√¥ng ph·∫£i zero\n",
    "        self.assertIsNotNone(self.Q.grad)\n",
    "        self.assertIsNotNone(self.K.grad)\n",
    "        self.assertIsNotNone(self.V.grad)\n",
    "        \n",
    "        # Ki·ªÉm tra gradients c√≥ magnitude > 0\n",
    "        self.assertGreater(self.Q.grad.abs().max().item(), 0)\n",
    "        self.assertGreater(self.K.grad.abs().max().item(), 0)\n",
    "        self.assertGreater(self.V.grad.abs().max().item(), 0)\n",
    "        \n",
    "        # Ki·ªÉm tra model parameters c√≥ gradients\n",
    "        for param in self.model.parameters():\n",
    "            self.assertIsNotNone(param.grad)\n",
    "            self.assertGreater(param.grad.abs().max().item(), 0)\n",
    "    \n",
    "    def test_attention_pattern_symmetry(self):\n",
    "        \"\"\"Test t√≠nh ch·∫•t ƒë·ªëi x·ª©ng khi Q=K\"\"\"\n",
    "        X = torch.randn(1, 5, self.d_model).to(self.device)\n",
    "        V = torch.randn(1, 5, self.d_model).to(self.device)\n",
    "        \n",
    "        _, attention_weights = self.model.forward_with_weights(X, X, V)\n",
    "        \n",
    "        # Khi Q=K, attention matrix n√™n c√≥ m·ªôt s·ªë t√≠nh ch·∫•t ƒë·ªëi x·ª©ng\n",
    "        # (kh√¥ng ho√†n to√†n ƒë·ªëi x·ª©ng v√¨ c√≥ linear transformations kh√°c nhau)\n",
    "        attention_matrix = attention_weights.squeeze(0)  # (5, 5)\n",
    "        \n",
    "        # √çt nh·∫•t diagonal elements n√™n c√≥ gi√° tr·ªã cao (self-attention)\n",
    "        diagonal_vals = torch.diag(attention_matrix)\n",
    "        off_diagonal_vals = attention_matrix[~torch.eye(5, dtype=bool)]\n",
    "        \n",
    "        # Trung b√¨nh diagonal n√™n >= trung b√¨nh off-diagonal\n",
    "        self.assertGreaterEqual(diagonal_vals.mean().item(), off_diagonal_vals.mean().item())\n",
    "    \n",
    "    def test_batch_consistency(self):\n",
    "        \"\"\"Test t√≠nh nh·∫•t qu√°n gi·ªØa c√°c batch\"\"\"\n",
    "        # T·∫°o input gi·ªëng nhau cho c·∫£ hai batch items\n",
    "        X = torch.randn(1, self.seq_len, self.d_model).to(self.device)\n",
    "        X_batched = X.repeat(2, 1, 1)  # (2, seq_len, d_model)\n",
    "        \n",
    "        output = self.model(X_batched, X_batched, X_batched)\n",
    "        \n",
    "        # Output c·ªßa batch item 0 v√† 1 n√™n gi·ªëng nhau\n",
    "        torch.testing.assert_close(output[0], output[1], rtol=1e-5, atol=1e-6)\n",
    "    \n",
    "    def test_numerical_stability(self):\n",
    "        \"\"\"Test t√≠nh ·ªïn ƒë·ªãnh s·ªë h·ªçc v·ªõi gi√° tr·ªã extreme\"\"\"\n",
    "        # Test v·ªõi gi√° tr·ªã l·ªõn\n",
    "        Q_large = torch.randn(2, 5, self.d_model).to(self.device) * 10\n",
    "        K_large = torch.randn(2, 5, self.d_model).to(self.device) * 10\n",
    "        V_large = torch.randn(2, 5, self.d_model).to(self.device) * 10\n",
    "        \n",
    "        output = self.model(Q_large, K_large, V_large)\n",
    "        \n",
    "        # Ki·ªÉm tra kh√¥ng c√≥ NaN ho·∫∑c Inf\n",
    "        self.assertFalse(torch.isnan(output).any())\n",
    "        self.assertFalse(torch.isinf(output).any())\n",
    "        \n",
    "        # Test v·ªõi gi√° tr·ªã nh·ªè\n",
    "        Q_small = torch.randn(2, 5, self.d_model).to(self.device) * 0.01\n",
    "        K_small = torch.randn(2, 5, self.d_model).to(self.device) * 0.01\n",
    "        V_small = torch.randn(2, 5, self.d_model).to(self.device) * 0.01\n",
    "        \n",
    "        output = self.model(Q_small, K_small, V_small)\n",
    "        \n",
    "        self.assertFalse(torch.isnan(output).any())\n",
    "        self.assertFalse(torch.isinf(output).any())\n",
    "    \n",
    "    def test_deterministic_output(self):\n",
    "        \"\"\"Test t√≠nh deterministic\"\"\"\n",
    "        torch.manual_seed(42)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(42)\n",
    "        \n",
    "        output1 = self.model(self.Q, self.K, self.V)\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(42)\n",
    "        \n",
    "        output2 = self.model(self.Q, self.K, self.V)\n",
    "        \n",
    "        torch.testing.assert_close(output1, output2)\n",
    "\n",
    "\n",
    "def run_comprehensive_test():\n",
    "    \"\"\"Ch·∫°y t·∫•t c·∫£ tests v·ªõi b√°o c√°o chi ti·∫øt\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COMPREHENSIVE TEST FOR PURE SCALED DOT-PRODUCT ATTENTION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # T·∫°o test suite\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestScaledDotProductAttention)\n",
    "    \n",
    "    # Ch·∫°y tests v·ªõi verbose output\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TEST SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Tests run: {result.testsRun}\")\n",
    "    print(f\"Failures: {len(result.failures)}\")\n",
    "    print(f\"Errors: {len(result.errors)}\")\n",
    "    print(f\"Success rate: {((result.testsRun - len(result.failures) - len(result.errors))/result.testsRun)*100:.1f}%\")\n",
    "    \n",
    "    return result.wasSuccessful()\n",
    "\n",
    "\n",
    "def demo_attention_visualization():\n",
    "    \"\"\"Demo v√† visualization c·ªßa attention mechanism\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ATTENTION MECHANISM DEMONSTRATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # T·∫°o model nh·ªè ƒë·ªÉ d·ªÖ quan s√°t\n",
    "    model = ScaledDotProductAttention(d_model=64)\n",
    "    \n",
    "    # T·∫°o m·ªôt sequence ƒë∆°n gi·∫£n\n",
    "    batch_size, seq_len = 1, 5\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    X = torch.randn(batch_size, seq_len, 64)\n",
    "    print(f\"Input shape: {X.shape}\")\n",
    "    \n",
    "    # Self-attention\n",
    "    print(f\"\\n--- SELF-ATTENTION ---\")\n",
    "    output, attention_weights = model.forward_with_weights(X, X, X)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "    print(f\"Attention matrix:\")\n",
    "    att_matrix = attention_weights.squeeze(0).detach().numpy()\n",
    "    for i in range(seq_len):\n",
    "        row_str = \" \".join([f\"{att_matrix[i, j]:.3f}\" for j in range(seq_len)])\n",
    "        print(f\"  Position {i}: [{row_str}]\")\n",
    "    \n",
    "    # V·ªõi causal mask\n",
    "    print(f\"\\n--- CAUSAL MASKED ATTENTION ---\")\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "    output_masked, attention_weights_masked = model.forward_with_weights(X, X, X, mask=mask)\n",
    "    print(f\"Masked attention matrix:\")\n",
    "    att_matrix_masked = attention_weights_masked.squeeze(0).detach().numpy()\n",
    "    for i in range(seq_len):\n",
    "        row_str = \" \".join([f\"{att_matrix_masked[i, j]:.3f}\" for j in range(seq_len)])\n",
    "        print(f\"  Position {i}: [{row_str}]\")\n",
    "    \n",
    "    # Cross-attention example\n",
    "    print(f\"\\n--- CROSS-ATTENTION ---\")\n",
    "    K_cross = torch.randn(batch_size, 3, 64)  # Shorter key/value sequence\n",
    "    V_cross = torch.randn(batch_size, 3, 64)\n",
    "    \n",
    "    output_cross, attention_weights_cross = model.forward_with_weights(X, K_cross, V_cross)\n",
    "    print(f\"Cross-attention output shape: {output_cross.shape}\")\n",
    "    print(f\"Cross-attention weights shape: {attention_weights_cross.shape}\")\n",
    "    \n",
    "    print(\"‚úì Demonstration completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ch·∫°y comprehensive test\n",
    "    success = run_comprehensive_test()\n",
    "    \n",
    "    # Ch·∫°y demonstration\n",
    "    demo_attention_visualization()\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\nüéâ All tests passed! Your Pure Scaled Dot-Product Attention is working correctly!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Some tests failed. Please check the output above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
