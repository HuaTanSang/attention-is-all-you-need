{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3075aa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_attention_pattern_symmetry (__main__.TestScaledDotProductAttention)\n",
      "Test tính chất đối xứng khi Q=K ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE TEST FOR PURE SCALED DOT-PRODUCT ATTENTION\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAIL\n",
      "test_attention_weights_properties (__main__.TestScaledDotProductAttention)\n",
      "Test các tính chất của attention weights ... ok\n",
      "test_batch_consistency (__main__.TestScaledDotProductAttention)\n",
      "Test tính nhất quán giữa các batch ... ok\n",
      "test_causal_mask (__main__.TestScaledDotProductAttention)\n",
      "Test với causal mask (cho decoder self-attention) ... ok\n",
      "test_deterministic_output (__main__.TestScaledDotProductAttention)\n",
      "Test tính deterministic ... ok\n",
      "test_forward_shape_cross_attention (__main__.TestScaledDotProductAttention)\n",
      "Test shape với cross-attention (K,V có seq_len khác Q) ... ok\n",
      "test_forward_shape_self_attention (__main__.TestScaledDotProductAttention)\n",
      "Test shape với self-attention (Q=K=V) ... ok\n",
      "test_gradient_flow (__main__.TestScaledDotProductAttention)\n",
      "Test gradient flow qua attention mechanism ... ok\n",
      "test_initialization (__main__.TestScaledDotProductAttention)\n",
      "Test khởi tạo model ... ok\n",
      "test_numerical_stability (__main__.TestScaledDotProductAttention)\n",
      "Test tính ổn định số học với giá trị extreme ... ok\n",
      "test_padding_mask (__main__.TestScaledDotProductAttention)\n",
      "Test với padding mask ... ok\n",
      "test_scaling_effect (__main__.TestScaledDotProductAttention)\n",
      "Test hiệu ứng của scaling factor sqrt(d_k) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_attention_pattern_symmetry (__main__.TestScaledDotProductAttention)\n",
      "Test tính chất đối xứng khi Q=K\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_17109/946854499.py\", line 232, in test_attention_pattern_symmetry\n",
      "    self.assertGreaterEqual(diagonal_vals.mean().item(), off_diagonal_vals.mean().item())\n",
      "AssertionError: 0.18042083084583282 not greater than or equal to 0.2048948109149933\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 1.967s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST SUMMARY\n",
      "======================================================================\n",
      "Tests run: 12\n",
      "Failures: 1\n",
      "Errors: 0\n",
      "Success rate: 91.7%\n",
      "\n",
      "======================================================================\n",
      "ATTENTION MECHANISM DEMONSTRATION\n",
      "======================================================================\n",
      "Input shape: torch.Size([1, 5, 64])\n",
      "\n",
      "--- SELF-ATTENTION ---\n",
      "Output shape: torch.Size([1, 5, 64])\n",
      "Attention weights shape: torch.Size([1, 5, 5])\n",
      "Attention matrix:\n",
      "  Position 0: [0.256 0.132 0.186 0.240 0.186]\n",
      "  Position 1: [0.171 0.176 0.243 0.149 0.261]\n",
      "  Position 2: [0.233 0.178 0.196 0.182 0.211]\n",
      "  Position 3: [0.173 0.213 0.175 0.282 0.156]\n",
      "  Position 4: [0.237 0.163 0.094 0.170 0.336]\n",
      "\n",
      "--- CAUSAL MASKED ATTENTION ---\n",
      "Masked attention matrix:\n",
      "  Position 0: [1.000 0.000 0.000 0.000 0.000]\n",
      "  Position 1: [0.494 0.506 0.000 0.000 0.000]\n",
      "  Position 2: [0.384 0.293 0.323 0.000 0.000]\n",
      "  Position 3: [0.205 0.253 0.207 0.335 0.000]\n",
      "  Position 4: [0.237 0.163 0.094 0.170 0.336]\n",
      "\n",
      "--- CROSS-ATTENTION ---\n",
      "Cross-attention output shape: torch.Size([1, 5, 64])\n",
      "Cross-attention weights shape: torch.Size([1, 5, 3])\n",
      "✓ Demonstration completed successfully!\n",
      "\n",
      "❌ Some tests failed. Please check the output above.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Linear transformations cho Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pure Scaled Dot-Product Attention\n",
    "        \n",
    "        Args:\n",
    "            Q: Query tensor (batch_size, seq_len_q, d_model)\n",
    "            K: Key tensor (batch_size, seq_len_k, d_model)  \n",
    "            V: Value tensor (batch_size, seq_len_v, d_model)\n",
    "            mask: Optional mask tensor (batch_size, seq_len_q, seq_len_k) hoặc broadcastable\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output (batch_size, seq_len_q, d_model)\n",
    "        \"\"\"\n",
    "        # Linear transformations\n",
    "        Q = self.W_q(Q)  # (batch_size, seq_len_q, d_model)\n",
    "        K = self.W_k(K)  # (batch_size, seq_len_k, d_model)\n",
    "        V = self.W_v(V)  # (batch_size, seq_len_v, d_model)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        d_k = K.size(-1)  # d_model\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # scores shape: (batch_size, seq_len_q, seq_len_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        # attention_weights shape: (batch_size, seq_len_q, seq_len_k)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        # output shape: (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forward_with_weights(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass với return cả attention weights (để debugging/visualization)\n",
    "        \"\"\"\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        d_k = K.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class TestScaledDotProductAttention(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Thiết lập các tham số test\"\"\"\n",
    "        self.batch_size = 2\n",
    "        self.seq_len = 10\n",
    "        self.d_model = 512\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Tạo model\n",
    "        self.model = ScaledDotProductAttention(self.d_model).to(self.device)\n",
    "        \n",
    "        # Tạo input tensors\n",
    "        self.Q = torch.randn(self.batch_size, self.seq_len, self.d_model).to(self.device)\n",
    "        self.K = torch.randn(self.batch_size, self.seq_len, self.d_model).to(self.device)\n",
    "        self.V = torch.randn(self.batch_size, self.seq_len, self.d_model).to(self.device)\n",
    "    \n",
    "    def test_initialization(self):\n",
    "        \"\"\"Test khởi tạo model\"\"\"\n",
    "        model = ScaledDotProductAttention(512)\n",
    "        self.assertEqual(model.d_model, 512)\n",
    "        self.assertIsInstance(model.W_q, nn.Linear)\n",
    "        self.assertIsInstance(model.W_k, nn.Linear)\n",
    "        self.assertIsInstance(model.W_v, nn.Linear)\n",
    "        \n",
    "        # Kiểm tra dimensions của linear layers\n",
    "        self.assertEqual(model.W_q.in_features, 512)\n",
    "        self.assertEqual(model.W_q.out_features, 512)\n",
    "    \n",
    "    def test_forward_shape_self_attention(self):\n",
    "        \"\"\"Test shape với self-attention (Q=K=V)\"\"\"\n",
    "        X = torch.randn(self.batch_size, self.seq_len, self.d_model).to(self.device)\n",
    "        output = self.model(X, X, X)\n",
    "        expected_shape = (self.batch_size, self.seq_len, self.d_model)\n",
    "        self.assertEqual(output.shape, expected_shape)\n",
    "    \n",
    "    def test_forward_shape_cross_attention(self):\n",
    "        \"\"\"Test shape với cross-attention (K,V có seq_len khác Q)\"\"\"\n",
    "        seq_len_kv = 15\n",
    "        K_cross = torch.randn(self.batch_size, seq_len_kv, self.d_model).to(self.device)\n",
    "        V_cross = torch.randn(self.batch_size, seq_len_kv, self.d_model).to(self.device)\n",
    "        \n",
    "        output = self.model(self.Q, K_cross, V_cross)\n",
    "        expected_shape = (self.batch_size, self.seq_len, self.d_model)  # shape theo Q\n",
    "        self.assertEqual(output.shape, expected_shape)\n",
    "    \n",
    "    def test_attention_weights_properties(self):\n",
    "        \"\"\"Test các tính chất của attention weights\"\"\"\n",
    "        output, attention_weights = self.model.forward_with_weights(self.Q, self.K, self.V)\n",
    "        \n",
    "        # 1. Shape của attention weights\n",
    "        expected_weights_shape = (self.batch_size, self.seq_len, self.seq_len)\n",
    "        self.assertEqual(attention_weights.shape, expected_weights_shape)\n",
    "        \n",
    "        # 2. Attention weights phải sum to 1 theo dim cuối\n",
    "        weights_sum = attention_weights.sum(dim=-1)\n",
    "        expected_sum = torch.ones(self.batch_size, self.seq_len).to(self.device)\n",
    "        torch.testing.assert_close(weights_sum, expected_sum, rtol=1e-5, atol=1e-6)\n",
    "        \n",
    "        # 3. Attention weights phải >= 0\n",
    "        self.assertTrue((attention_weights >= 0).all())\n",
    "        \n",
    "        # 4. Attention weights phải <= 1\n",
    "        self.assertTrue((attention_weights <= 1).all())\n",
    "    \n",
    "    def test_causal_mask(self):\n",
    "        \"\"\"Test với causal mask (cho decoder self-attention)\"\"\"\n",
    "        # Tạo causal mask (lower triangular)\n",
    "        mask = torch.tril(torch.ones(self.seq_len, self.seq_len))\n",
    "        mask = mask.unsqueeze(0).expand(self.batch_size, -1, -1).to(self.device)\n",
    "        \n",
    "        output, attention_weights = self.model.forward_with_weights(self.Q, self.K, self.V, mask=mask)\n",
    "        \n",
    "        # Kiểm tra attention weights = 0 ở vị trí mask = 0\n",
    "        masked_positions = (mask == 0)\n",
    "        masked_weights = attention_weights[masked_positions]\n",
    "        \n",
    "        # Attention weights tại vị trí masked phải rất nhỏ (≈ 0)\n",
    "        self.assertTrue((masked_weights < 1e-8).all())\n",
    "    \n",
    "    def test_padding_mask(self):\n",
    "        \"\"\"Test với padding mask\"\"\"\n",
    "        # Giả sử token cuối cùng là padding\n",
    "        mask = torch.ones(self.batch_size, self.seq_len, self.seq_len).to(self.device)\n",
    "        mask[:, :, -1] = 0  # Mask token cuối cùng\n",
    "        \n",
    "        output, attention_weights = self.model.forward_with_weights(self.Q, self.K, self.V, mask=mask)\n",
    "        \n",
    "        # Attention weights tại cột cuối phải ≈ 0\n",
    "        last_column_weights = attention_weights[:, :, -1]\n",
    "        self.assertTrue((last_column_weights < 1e-8).all())\n",
    "    \n",
    "    def test_scaling_effect(self):\n",
    "        \"\"\"Test hiệu ứng của scaling factor sqrt(d_k)\"\"\"\n",
    "        # So sánh với attention không scale\n",
    "        Q_proj = self.model.W_q(self.Q)\n",
    "        K_proj = self.model.W_k(self.K)\n",
    "        V_proj = self.model.W_v(self.V)\n",
    "        \n",
    "        # Attention có scale\n",
    "        scores_scaled = torch.matmul(Q_proj, K_proj.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "        weights_scaled = F.softmax(scores_scaled, dim=-1)\n",
    "        \n",
    "        # Attention không scale\n",
    "        scores_unscaled = torch.matmul(Q_proj, K_proj.transpose(-2, -1))\n",
    "        weights_unscaled = F.softmax(scores_unscaled, dim=-1)\n",
    "        \n",
    "        # Scaled attention weights nên ít concentrated hơn (entropy cao hơn)\n",
    "        def entropy(weights):\n",
    "            return -(weights * torch.log(weights + 1e-9)).sum(dim=-1)\n",
    "        \n",
    "        entropy_scaled = entropy(weights_scaled).mean()\n",
    "        entropy_unscaled = entropy(weights_unscaled).mean()\n",
    "        \n",
    "        # Với d_model lớn, scaled attention thường có entropy cao hơn\n",
    "        if self.d_model > 64:\n",
    "            self.assertGreater(entropy_scaled, entropy_unscaled)\n",
    "    \n",
    "    def test_gradient_flow(self):\n",
    "        \"\"\"Test gradient flow qua attention mechanism\"\"\"\n",
    "        self.Q.requires_grad_(True)\n",
    "        self.K.requires_grad_(True)\n",
    "        self.V.requires_grad_(True)\n",
    "        \n",
    "        output = self.model(self.Q, self.K, self.V)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Kiểm tra gradients không phải None và không phải zero\n",
    "        self.assertIsNotNone(self.Q.grad)\n",
    "        self.assertIsNotNone(self.K.grad)\n",
    "        self.assertIsNotNone(self.V.grad)\n",
    "        \n",
    "        # Kiểm tra gradients có magnitude > 0\n",
    "        self.assertGreater(self.Q.grad.abs().max().item(), 0)\n",
    "        self.assertGreater(self.K.grad.abs().max().item(), 0)\n",
    "        self.assertGreater(self.V.grad.abs().max().item(), 0)\n",
    "        \n",
    "        # Kiểm tra model parameters có gradients\n",
    "        for param in self.model.parameters():\n",
    "            self.assertIsNotNone(param.grad)\n",
    "            self.assertGreater(param.grad.abs().max().item(), 0)\n",
    "    \n",
    "    def test_attention_pattern_symmetry(self):\n",
    "        \"\"\"Test tính chất đối xứng khi Q=K\"\"\"\n",
    "        X = torch.randn(1, 5, self.d_model).to(self.device)\n",
    "        V = torch.randn(1, 5, self.d_model).to(self.device)\n",
    "        \n",
    "        _, attention_weights = self.model.forward_with_weights(X, X, V)\n",
    "        \n",
    "        # Khi Q=K, attention matrix nên có một số tính chất đối xứng\n",
    "        # (không hoàn toàn đối xứng vì có linear transformations khác nhau)\n",
    "        attention_matrix = attention_weights.squeeze(0)  # (5, 5)\n",
    "        \n",
    "        # Ít nhất diagonal elements nên có giá trị cao (self-attention)\n",
    "        diagonal_vals = torch.diag(attention_matrix)\n",
    "        off_diagonal_vals = attention_matrix[~torch.eye(5, dtype=bool)]\n",
    "        \n",
    "        # Trung bình diagonal nên >= trung bình off-diagonal\n",
    "        self.assertGreaterEqual(diagonal_vals.mean().item(), off_diagonal_vals.mean().item())\n",
    "    \n",
    "    def test_batch_consistency(self):\n",
    "        \"\"\"Test tính nhất quán giữa các batch\"\"\"\n",
    "        # Tạo input giống nhau cho cả hai batch items\n",
    "        X = torch.randn(1, self.seq_len, self.d_model).to(self.device)\n",
    "        X_batched = X.repeat(2, 1, 1)  # (2, seq_len, d_model)\n",
    "        \n",
    "        output = self.model(X_batched, X_batched, X_batched)\n",
    "        \n",
    "        # Output của batch item 0 và 1 nên giống nhau\n",
    "        torch.testing.assert_close(output[0], output[1], rtol=1e-5, atol=1e-6)\n",
    "    \n",
    "    def test_numerical_stability(self):\n",
    "        \"\"\"Test tính ổn định số học với giá trị extreme\"\"\"\n",
    "        # Test với giá trị lớn\n",
    "        Q_large = torch.randn(2, 5, self.d_model).to(self.device) * 10\n",
    "        K_large = torch.randn(2, 5, self.d_model).to(self.device) * 10\n",
    "        V_large = torch.randn(2, 5, self.d_model).to(self.device) * 10\n",
    "        \n",
    "        output = self.model(Q_large, K_large, V_large)\n",
    "        \n",
    "        # Kiểm tra không có NaN hoặc Inf\n",
    "        self.assertFalse(torch.isnan(output).any())\n",
    "        self.assertFalse(torch.isinf(output).any())\n",
    "        \n",
    "        # Test với giá trị nhỏ\n",
    "        Q_small = torch.randn(2, 5, self.d_model).to(self.device) * 0.01\n",
    "        K_small = torch.randn(2, 5, self.d_model).to(self.device) * 0.01\n",
    "        V_small = torch.randn(2, 5, self.d_model).to(self.device) * 0.01\n",
    "        \n",
    "        output = self.model(Q_small, K_small, V_small)\n",
    "        \n",
    "        self.assertFalse(torch.isnan(output).any())\n",
    "        self.assertFalse(torch.isinf(output).any())\n",
    "    \n",
    "    def test_deterministic_output(self):\n",
    "        \"\"\"Test tính deterministic\"\"\"\n",
    "        torch.manual_seed(42)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(42)\n",
    "        \n",
    "        output1 = self.model(self.Q, self.K, self.V)\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(42)\n",
    "        \n",
    "        output2 = self.model(self.Q, self.K, self.V)\n",
    "        \n",
    "        torch.testing.assert_close(output1, output2)\n",
    "\n",
    "\n",
    "def run_comprehensive_test():\n",
    "    \"\"\"Chạy tất cả tests với báo cáo chi tiết\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COMPREHENSIVE TEST FOR PURE SCALED DOT-PRODUCT ATTENTION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Tạo test suite\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestScaledDotProductAttention)\n",
    "    \n",
    "    # Chạy tests với verbose output\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TEST SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Tests run: {result.testsRun}\")\n",
    "    print(f\"Failures: {len(result.failures)}\")\n",
    "    print(f\"Errors: {len(result.errors)}\")\n",
    "    print(f\"Success rate: {((result.testsRun - len(result.failures) - len(result.errors))/result.testsRun)*100:.1f}%\")\n",
    "    \n",
    "    return result.wasSuccessful()\n",
    "\n",
    "\n",
    "def demo_attention_visualization():\n",
    "    \"\"\"Demo và visualization của attention mechanism\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ATTENTION MECHANISM DEMONSTRATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Tạo model nhỏ để dễ quan sát\n",
    "    model = ScaledDotProductAttention(d_model=64)\n",
    "    \n",
    "    # Tạo một sequence đơn giản\n",
    "    batch_size, seq_len = 1, 5\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    X = torch.randn(batch_size, seq_len, 64)\n",
    "    print(f\"Input shape: {X.shape}\")\n",
    "    \n",
    "    # Self-attention\n",
    "    print(f\"\\n--- SELF-ATTENTION ---\")\n",
    "    output, attention_weights = model.forward_with_weights(X, X, X)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "    print(f\"Attention matrix:\")\n",
    "    att_matrix = attention_weights.squeeze(0).detach().numpy()\n",
    "    for i in range(seq_len):\n",
    "        row_str = \" \".join([f\"{att_matrix[i, j]:.3f}\" for j in range(seq_len)])\n",
    "        print(f\"  Position {i}: [{row_str}]\")\n",
    "    \n",
    "    # Với causal mask\n",
    "    print(f\"\\n--- CAUSAL MASKED ATTENTION ---\")\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "    output_masked, attention_weights_masked = model.forward_with_weights(X, X, X, mask=mask)\n",
    "    print(f\"Masked attention matrix:\")\n",
    "    att_matrix_masked = attention_weights_masked.squeeze(0).detach().numpy()\n",
    "    for i in range(seq_len):\n",
    "        row_str = \" \".join([f\"{att_matrix_masked[i, j]:.3f}\" for j in range(seq_len)])\n",
    "        print(f\"  Position {i}: [{row_str}]\")\n",
    "    \n",
    "    # Cross-attention example\n",
    "    print(f\"\\n--- CROSS-ATTENTION ---\")\n",
    "    K_cross = torch.randn(batch_size, 3, 64)  # Shorter key/value sequence\n",
    "    V_cross = torch.randn(batch_size, 3, 64)\n",
    "    \n",
    "    output_cross, attention_weights_cross = model.forward_with_weights(X, K_cross, V_cross)\n",
    "    print(f\"Cross-attention output shape: {output_cross.shape}\")\n",
    "    print(f\"Cross-attention weights shape: {attention_weights_cross.shape}\")\n",
    "    \n",
    "    print(\"✓ Demonstration completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Chạy comprehensive test\n",
    "    success = run_comprehensive_test()\n",
    "    \n",
    "    # Chạy demonstration\n",
    "    demo_attention_visualization()\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\n🎉 All tests passed! Your Pure Scaled Dot-Product Attention is working correctly!\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Some tests failed. Please check the output above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
